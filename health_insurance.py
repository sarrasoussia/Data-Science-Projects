# -*- coding: utf-8 -*-
"""health insurance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cTacui31cMFi-tEk-LPKzk5jguJECNB1
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

#healthcare insurance dataset
dataset=pd.read_csv("insurance.csv")
dataset.tail()

dataset.isnull().sum().sum()



dataset.dtypes[::]

dataset.info()

dataset.describe()

dataset = pd.DataFrame(np.random.rand(10, 5), columns=['age','sex','bmi','children','smoker'])
dataset.plot.box(grid='True')

dataset.corr()

"""a negative corr between sex and age"""

def plot_correlation_map( df ):
    corr = df.corr()
    s , ax = plt.subplots( figsize =( 12 , 10 ) )
    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )
    s = sns.heatmap(
        corr,
        cmap = cmap,
        square=True,
        cbar_kws={ 'shrink' : .9 },
        ax=ax,
        annot = True,
        annot_kws = { 'fontsize' : 12 }
        )
plot_correlation_map( dataset )

"""*   a remarquable corr between sex and bmi/ age and children
*   a negative corr betweeb bmi and smoking / sex and smoking


"""

dataset['age'].plot(kind='density', figsize=(14,6))

dataset.groupby(by='smoker')['charges'].mean()

"""smokers have larger charges than non smokers"""

dataset.groupby(by='sex')['charges'].mean()

"""males have slightly higher charges than females"""



grid= sns.FacetGrid(dataset, col='smoker')
grid.map(sns.barplot,'sex','age')
grid.set_xticklabels(rotation = 25)
grid.add_legend()

plt.xlabel("charges")
dataset['charges'].plot.hist()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

#features extraction
x=dataset[["PassengerId","Pclass"]]
y=dataset["Survived"]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.35,random_state=40) #splitting data with test size of 35%

logreg = LogisticRegression()   #build our logistic model
logreg.fit(x_train, y_train)  #fitting training data
y_pred  = logreg.predict(x_test)    #testing modelâ€™s performance
print("Accuracy={:.2f}".format(logreg.score(x_test, y_test)))

X=dataset[["age","bmi","smoker","children"]]
#Plotting Elbow Curve
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from sklearn import metrics
#fitting into the model
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,10))

visualizer.fit(X)

X=dataset[["age","bmi","smoker","children","sex"]]
km=KMeans(n_clusters=2,random_state=42)
km.fit(X)
print("Cluster centers for first   -->",km.cluster_centers_[0])
print("Cluster centers for second  -->",km.cluster_centers_[1])
print("clustering labels are :",km.labels_[:10])

pd.Series(km.labels_).value_counts()

k_range = range(2, 10)
X=dataset[["age","bmi","smoker","children","sex"]]
scores = []
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X)
    scores.append(metrics.silhouette_score(X, km.labels_))
print("Score from 2 to 10 -> \n",scores[::])
print("Highest value in silhouette score is of cluster 2 --> ",scores[0])

