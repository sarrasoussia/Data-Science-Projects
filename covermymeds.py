# -*- coding: utf-8 -*-
"""CoverMyMeds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MqHLPH_eUU6ojg0pvIUj6uPBhiNS1AEY

Today we will explore a data set dedicated to the cost of treatment of different patients. The cost of treatment depends on many factors: diagnosis, s*x, city of residence, age and so on. We have no data on the diagnosis of patients. But we have other information that can help us to make a conclusion about the health of patients and practice regression analysis. In any case, I wish you to be healthy! Let's look at our data.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
df = pd.read_csv ('insurance.csv',sep=",")

df.tail()

df.info()

df.describe()

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder
#sex
le = LabelEncoder()
le.fit(df.sex.drop_duplicates())
df.sex = le.transform(df.sex)
# smoker or not
le.fit(df.smoker.drop_duplicates())
df.smoker = le.transform(df.smoker)
#region
le.fit(df.region.drop_duplicates())
df.region = le.transform(df.region)

df

"""0 for female and 1 for male

---
smoker 1 and non smoker 0

---
northeast = 0
northwest = 1
souteast = 2
southwest = 3



"""

import matplotlib.pyplot as plt
import warnings
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score,mean_squared_error

mm=sns.catplot(x="smoker", kind="count",hue = 'sex', palette="pink", data=df)

"""non smoker females are greater than non smoker males, and are both greater than 500

---
males who smoke are greater than female smokers, varies between 100 and 200

"""

mm=sns.catplot(x="children", kind="count",hue = 'smoker', palette="Pastel1", data=df)

"""having children decreases the probability of being a smoker"""

f= plt.figure(figsize=(12,5))

ax=f.add_subplot(121)
sns.distplot(df[(df.smoker == 1)]["charges"],color='c',ax=ax)
ax.set_title('Distribution of charges for smokers')

ax=f.add_subplot(122)
sns.distplot(df[(df.smoker == 0)]['charges'],color='b',ax=ax)
ax.set_title('Distribution of charges for non-smokers')

vv=sns.catplot(x="sex", y="charges", hue="smoker",
            kind="violin", data=df, palette = 'magma')

"""smokers generally have greater charges than those who don't smoke"""

plt.figure(figsize=(12,5))
plt.title("Box plot for charges of women")
mm=sns.boxplot(y="smoker", x="charges", data =  df[(df.sex == 0)] , orient="h", palette = 'gist_rainbow')

plt.figure(figsize=(12,5))
plt.title("Box plot for charges of men")
mm=sns.boxplot(y="smoker", x="charges", data =  df[(df.sex == 1)] , orient="h", palette = 'rainbow')

"""Now let s pay attention to the age of the patients. First, let s look at how age affects the cost of treatment, and also look at patients of what age more in our data set."""

plt.figure(figsize=(12,5))
plt.title("Distribution of age")
ax = sns.distplot(df["age"], color = 'g')

#20 yo are more present
mm=sns.catplot(x="smoker", kind="count",hue = 'sex', palette="rainbow", data=df[(df.age == 20)])
plt.title("The number of smokers and non-smokers (20 years old)")

plt.figure(figsize=(12,5))
plt.title("Box plot for charges 20 years old smokers")
mm=sns.boxplot(y="smoker", x="charges", data = df[(df.age == 20)] , orient="h", palette = 'pink')

"""As we can see, even at the age of 20 smokers spend much more on treatment than non-smokers. we can conclude that there s high correlation between smoking, age and charges.

---

average charges = 20000
"""

sns.lmplot(x="age", y="charges", hue="smoker", data=df, palette = 'inferno_r', size = 10)
ax.set_title('Smokers and non-smokers')

"""As we can see, the increase of charges with age is quite certain however when it comes to the case of smokers we see a much greater amount of charges than those who don't smoke."""

plt.figure(figsize=(12,5))
plt.title("Distribution of bmi")
ax = sns.distplot(df["bmi"], color = 'r')

#Turning BMI into Categorical Variables:

df["weight_condition"] = np.nan
lst = [df]

for col in lst:
    col.loc[col["bmi"] < 18.5, "weight_condition"] = "Underweight"
    col.loc[(col["bmi"] >= 18.5) & (col["bmi"] < 24.986), "weight_condition"] = "Normal Weight"
    col.loc[(col["bmi"] >= 25) & (col["bmi"] < 29.926), "weight_condition"] = "Overweight"
    col.loc[col["bmi"] >= 30, "weight_condition"] = "Obese"

df.head()

le.fit(df.weight_condition.drop_duplicates())
df.weight_condition = le.transform(df.weight_condition)
df.head()

"""normal weight = 0
obese = 1
overweight = 2
underweight = 3
"""

plt.figure(figsize=(10,6))
ax = sns.scatterplot(x='bmi',y='charges',data=df,palette='magma',hue='smoker')
ax.set_title('Scatter plot of charges and bmi')

mm=sns.lmplot(x="bmi", y="charges", hue="smoker", data=df, palette = 'magma', size = 8)

vv=sns.catplot(x="region", y="charges", hue="smoker",
            kind="violin", data=df, palette = 'magma')

"""Based from above plot, we can disclose that region of origin doesn t have much impact with the amount of medical cost."""

plt.figure(figsize=(10,10))
sns.heatmap(df.corr(), vmin=-1, cmap='coolwarm', annot=True);

"""a high corr between smokers and charges and a slightly high corr between mbi-age and charges"""

obese_smoker = df.loc[df['weight_condition'] == 2].loc[df['smoker'] == 1]
obese_nosmoker = df.loc[df['weight_condition'] == 2].loc[df['smoker'] == 0]

obese_smoker_skew = round(obese_smoker['charges'].skew(), 3)
just_obese_skew = round(obese_nosmoker['charges'].skew(), 3)

f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,8))

sns.distplot(obese_smoker['charges'], bins=50, ax=ax1, color='skyblue')
ax1.set_title(f'Smoker Obese {obese_smoker_skew}',fontsize=14)

sns.distplot(obese_nosmoker['charges'], bins=50, ax=ax2, color='#E91E63')
ax2.set_title(f'Just Obese {just_obese_skew}', fontsize=14)

plt.figure(figsize=(10,6))
sns.scatterplot(x='age', y='charges', hue='weight_condition', data=df, palette='muted',legend="full",s=80, alpha=.7)
plt.title('Age vs Charges', fontsize=14)
plt.show()
#0 for the overweight
#1 for obese
#2 normal
#3 underweight

plt.figure(figsize=(12,20))
ax = sns.lmplot(x = 'age', y = 'charges', data=df, hue='smoker')

ax = sns.lmplot(x = 'children', y = 'charges', data=df, hue='smoker')

ax = sns.lmplot(x = 'bmi', y = 'charges', data=df, hue='smoker')

df.groupby(by='sex')['charges'].mean()















"""Training & Test Dataset Split, Creation The First Model"""

#Train Test split
X = df.drop(['charges','region', 'sex', 'children'], axis = 1)
y = df.charges

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)

#Linear Regression
linear_reg = LinearRegression()
linear_reg.fit(X_train,y_train)
y_test_pred = linear_reg.predict(X_test)
print(linear_reg.score(X_test,y_test))

#Random Forest Regressor
random_reg = RandomForestRegressor(random_state = 2)
random_reg.fit(X_train,y_train)
random_test_pred = random_reg.predict(X_test)
print(random_reg.score(X_test,y_test))

#import relevant libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

#features extraction
x=df[["charges"]]
y=df["sex"]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.35,random_state=40) #splitting data with test size of 35%

logreg = LogisticRegression()   #build our logistic model
logreg.fit(x_train, y_train)  #fitting training data
y_pred  = logreg.predict(x_test)    #testing model’s performance
print("Accuracy={:.2f}".format(logreg.score(x_test, y_test)))

x=df[["charges"]]
y=df["smoker"]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.35,random_state=40) #splitting data with test size of 35%

logreg = LogisticRegression()   #build our logistic model
logreg.fit(x_train, y_train)  #fitting training data
y_pred  = logreg.predict(x_test)    #testing model’s performance
print("Accuracy={:.2f}".format(logreg.score(x_test, y_test)))

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn import metrics

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .20, random_state = 42)

logreg = LogisticRegression()
logreg.fit(x_train, y_train)

y_pred = logreg.predict(x_test)

acc_log = round(logreg.score(x_train, y_train) * 100, 2)



confusion_matrix = pd.crosstab(y_test,y_pred, rownames=['sex'], colnames=['smoker'])
print( confusion_matrix)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

#Choosing the features as x and the target as y
x=df[['charges']]
y=df['smoker']

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=30) #split our data with test size of 20%

knn=KNeighborsClassifier(n_neighbors=1000) #build our knn classifier
knn.fit(x_train,y_train) #Training KNN classifier
y_pred=knn.predict(x_test)  #Testing
print('Acuuracy=',accuracy_score(y_pred,y_test))

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score,mean_squared_error
from sklearn.ensemble import RandomForestRegressor
x = df.drop(['charges'], axis = 1)
y = df.charges

x_train,x_test,y_train,y_test = train_test_split(x,y, random_state = 0)
lr = LinearRegression().fit(x_train,y_train)

y_train_pred = lr.predict(x_train)
y_test_pred = lr.predict(x_test)

print(lr.score(x_test,y_test))

"""Not bad for such a lazy implementation, even without data normalization:D After all, the data will not always be so "good"."""

X = df.drop(['charges','region'], axis = 1)
Y = df.charges



quad = PolynomialFeatures (degree = 2)
x_quad = quad.fit_transform(X)

X_train,X_test,Y_train,Y_test = train_test_split(x_quad,Y, random_state = 0)

plr = LinearRegression().fit(X_train,Y_train)

Y_train_pred = plr.predict(X_train)
Y_test_pred = plr.predict(X_test)

print(plr.score(X_test,Y_test))

"""Already good. Our model predicts well the cost of treatment of patients."""

forest = RandomForestRegressor(n_estimators = 100,
                              criterion = 'mse',
                              random_state = 1,
                              n_jobs = -1)
forest.fit(x_train,y_train)
forest_train_pred = forest.predict(x_train)
forest_test_pred = forest.predict(x_test)

print('MSE train data: %.3f, MSE test data: %.3f' % (
mean_squared_error(y_train,forest_train_pred),
mean_squared_error(y_test,forest_test_pred)))
print('R2 train data: %.3f, R2 test data: %.3f' % (
r2_score(y_train,forest_train_pred),
r2_score(y_test,forest_test_pred)))

import matplotlib.pyplot as pl

pl.figure(figsize=(10,6))

pl.scatter(forest_train_pred,forest_train_pred - y_train,
          c = 'black', marker = 'o', s = 35, alpha = 0.5,
          label = 'Train data')
pl.scatter(forest_test_pred,forest_test_pred - y_test,
          c = 'c', marker = 'o', s = 35, alpha = 0.7,
          label = 'Test data')
pl.xlabel('Predicted values')
pl.ylabel('Tailings')
pl.legend(loc = 'upper left')
pl.hlines(y = 0, xmin = 0, xmax = 60000, lw = 2, color = 'red')
pl.show()

